# Reversi

リバーシのAIをJavaScriptでつくってみたものです

## 概要

αβ法による探索とシンプルな評価関数で作られています  
単純でありながらそこそこ強いものに仕上がったと思います

# 詳細

## 評価関数

本実装の評価関数は、シンプルな全結合ニューラルネットワーク（MLP）です。盤面をチャネル化してベクトル化し、実数スコア（手番視点）を出力します。

- 入力特徴（64マス × 5チャネル）
	- 手番側の石
	- 相手側の石
	- 空きマス
	- 手番の合法手
	- 相手番の合法手
- ネットワーク構成
	- 隠れ層は任意の深さ・ユニット数（既定は [16, 8]）
	- 隠れ層活性化: tanh、出力層: 線形
	- ステージ別パラメータ（石数 0..64 で別重みを保持）
- 学習
	- 自己対戦で得た教師信号に対し二乗誤差を最小化
	- Optimizer: Adam（既定）/SGD、L2 正則化に対応
- モデル保存形式
	- JSON（`model.nn.json`、バージョン3）。旧形式や4チャネルモデルは読み込み時に自動で移行・拡張します

実装は `src/evaluate.js` の `NNEval` クラスです（互換用に従来の線形 `Eval` も残しています）。

## 探索

αβ法をベースに、以下の効率化を入れています。

- 反復深化（Iterative Deepening）
	- 浅い深さから順に反復し、直前反復の最善手を次反復のルート順序付けに利用
- アスピレーションウィンドウ
	- 直前反復のスコア周辺に狭いウィンドウを設定し、失敗時のみ段階的に広げて再探索
- トランスポジションテーブル（TT）
	- Zobristハッシュ（`board.zkey`）をキーに、値・境界・深さ・最善手を保存/再利用
- ムーブオーダリング
	- ルートでは前回最善手を優先、ノード内部ではTT格納の最善手を先頭に

ブラウザ版では探索はWebWorker上で実行し、UIスレッドと分離しています。実装は `src/search.js` です。

# Licence
GPL v3
