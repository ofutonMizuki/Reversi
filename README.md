# Reversi

リバーシのAIをJavaScriptでつくってみたものです

## 概要

αβ法による探索とシンプルな評価関数で作られています  
単純でありながらそこそこ強いものに仕上がったと思います

# 詳細

## 評価関数

本実装の評価関数は、シンプルな全結合ニューラルネットワーク（MLP）です。盤面をチャネル化してベクトル化し、実数スコア（手番視点）を出力します。

- 入力特徴（64マス × 5チャネル）
	- 手番側の石
	- 相手側の石
	- 空きマス
	- 手番の合法手
	- 相手番の合法手
- ネットワーク構成
	- 隠れ層は任意の深さ・ユニット数（既定は [16, 8]）
	- 隠れ層活性化: tanh、出力層: 線形
	- ステージ別パラメータ（石数 0..64 で別重みを保持）
- 学習
	- 自己対戦で得た教師信号に対し二乗誤差を最小化
	- Optimizer: Adam（既定）/SGD、L2 正則化に対応
- モデル保存形式
	- JSON（`model.nn.json`、バージョン3）。旧形式や4チャネルモデルは読み込み時に自動で移行・拡張します

実装は `src/evaluate.js` の `NNEval` クラスです（互換用に従来の線形 `Eval` も残しています）。

## 探索

αβ探索をベースに、結果を変えない範囲の高速化を多数入れています（実装は `src/search.js`）。

- 反復深化（Iterative Deepening）
	- 深さ2から目標深さまで段階的に深めます。各反復の最善手を次の反復の初期順序付けに活用します。
- アスピレーションウィンドウ
	- 直前反復のスコアを中心に狭いウィンドウで探索し、fail-low/high 時のみウィンドウを倍々に拡張して再検索します。
- トランスポジションテーブル（TT）＋Zobristハッシュ
	- 64bit Zobrist（BigInt）をキーに、スコア・境界種別（EXACT/LOWER/UPPER）・残り深さ・最善手を保存。再訪時の枝刈りや順序付けに利用します。
- PVS（Principal Variation Search）
	- 各ノード2手目以降はまず null-window で探索し、ウィンドウを突破した場合のみフルウィンドウで再検索します。
- キラームーブ／ヒストリーヒューリスティック
	- 同一手番深さでカットを生んだ手（killer）や履歴で良かった手（history）を優先します。
- LMR（Late Move Reductions）
	- 明確に遅い手（並びで後方の手）を浅く探索して判定が変わる場合のみ掘り直します。
- 静的ムーブ順序
	- 角を大きく優遇、X/Cマスは減点。これに killer/history/TT の情報を合算して降順ソートします。
- NN評価の増分更新（インクリメンタル）
	- 盤面の絶対チャネル（自石/敵石/空き＋双方の合法手）をキャッシュし、子局面は差分だけを更新して `evaluateFromAbs` で高速評価します。
- 合法手マスクの再利用とパス高速判定
	- 子局面の `posBoard` を片側合法手として再利用し、もう片側のみ計算。合法手マスクの有無でパスを即判定します。

出力は最善手・評価値（手番視点）・探索ノード数を返します。ブラウザ版では探索は Web Worker 上で動作し、UI スレッドと分離しています。

# Licence
GPL v3
